\documentclass[a4paper,dvipdfmx]{jsarticle}
\usepackage{graphicx}
\usepackage{fancybox}
\usepackage{ascmac}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{comment}
\usepackage{here}
\usepackage{listings,jlisting}
\usepackage[a4paper,margin=2cm,truedimen]{geometry}
\usepackage{amssymb}
\usepackage[svgnames]{xcolor}

  \definecolor{diffstart}{named}{Grey}
  \definecolor{diffincl}{named}{Green}
  \definecolor{diffrem}{named}{OrangeRed}

  \lstdefinelanguage{diff}{
    basicstyle=\ttfamily\small,
    morecomment=[f][\color{diffstart}]{@@},
    morecomment=[f][\color{diffincl}]{+\ },
    morecomment=[f][\color{diffrem}]{-\ },
  }

\newcommand{\image}[3]{
    \begin{figure}[H]
        \begin{center}
        \includegraphics[bb=0 0 480 360,width=14cm]{#2}
        \end{center}
        \caption{#1}
        \label{#3}
    \end{figure}
}

\lstset{
	%language=[Objective]Caml,
	language=Python,
    basicstyle=\ttfamily,
    keywordstyle=\color[RGB]{33,74,180}\bfseries,
    stringstyle=\color[RGB]{240,100,5},
    commentstyle=\color[RGB]{100,100,100}\itshape,
    numberstyle=\footnotesize,
    numbers=left,
    stepnumber=1,
    numbersep=15pt,
    backgroundcolor=\color[RGB]{251,251,251},
    frame=single,
    frameround=ffff,
    framesep=5pt,
    rulecolor=\color[RGB]{128,128,128}, 
    breaklines=true,
    breakautoindent=true,
    breakatwhitespace=true,
    breakindent=25pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    captionpos=b,
    linewidth=\textwidth,
}
%\end{comment}
\begin{document}
% タイトルはここ
%\title{機能設計仕様書}
% 自分の情報はここに
%\author{工学部情報学科計算機科学コース \\ 1029-28-8969 渡邊 綾仁}

% 提出日って書いているけどコンパイルした日
% 使いたくなかったらコメントアウト

% \date{提出日: \today}

%\maketitle

% 以下、コピペ用
% includegraphicsとかtableとかめんどくさいのでここに書いておくとよさそう

\begin{comment}
%コメントアウトここから
% ----------------

%画像挿入用
\begin{figure}[H]
\begin{center}
\includegraphics[width=14cm]{hoge.png}
\end{center}
\caption{キャプション}
\label{参照ラベル}
\end{figure}

%ソースコード挿入用
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
\end{lstlisting}

%table環境
\begin{table}[H]
\begin{center}
\caption{キャプション}
  \begin{tabular}{|c|c|c|c|} \hline
    ノード名 & 割り当てたピンの番号 & 信号名 & スイッチ名 \\ \hline \hline
  \end{tabular}
	\label{ラベル}
\end{center}
\end{table}


%コメントアウトおしまい
\end{comment}

%20180506 メモ
\begin{comment}
simple.pdfのSIMPLE/Bの記述が、方式設計レベルの仕様書に該当するようなので、
このような記述方法を参考にすると、多分system_design_spec.pdfは書きやすいかも。
\end{comment}
%メモおしまい

% ----------------
%ここから本文
\section*{課題4}

\subsection*{課題内容}

MNIST のテスト画像1 枚を入力とし，3 層ニューラルネットワークを用いて，0～9 の値のうち1 つを出力するプログラムを作成せよ．

\subsection*{作成したプログラムについて}

課題1 で作成した my\_nn\_test.py に改良を加えた。
課題1から追加、変更になった点は以下の通りである。

\begin{itemize}
	\item 課題2, 3において作成した、学習後のパラメータを記録したファイルを読み込み、識別を行う
	\item 標準入力から不正な値が入力された場合、ファイルが存在しなかった場合などの例外処理
\end{itemize}

\begin{enumerate}
	\item NNTest クラスインスタンスの作成
	\item 学習したパラメータを利用して識別を行う場合は、ファイル名を標準入力から入力、課題1のようにランダムに初期化されたパラメータを用いて識別を行う場合は何も入力せずに進む。ファイルが存在しない場合はエラーとする。\\
	・パラメータをファイルから読み込んだ場合
	\begin{enumerate}
		\item パラメータをファイルから読み込んだ場合は、インスタンス作成時に初期化した各パラメータを読み込んだパラメータに書き換える。
		\item 中間層の活性化関数をシグモイド関数、ReLU、Dropout、ReLU + Batch normalization からパラメータファイルの内容に応じて選択する。 
		\item テスト回数を1〜1000の間で設定する。ここでテスト回数を1に設定すると、課題4の仕様の通り、画像1枚を入力（0〜9999の整数を1つ入力）として識別を行い、正解ラベルと認識結果を標準出力に出力する。
		\item テスト回数が2回以上の場合は、テストデータ10000枚の画像から1回あたり100枚の画像をランダムに選んで識別を行い、Accuracy を計算する。終了後、テスト回数とAccuracy の関係のグラフを出力し、標準出力に最終的なAccuracy の値を出力する。
	\end{enumerate}
	・パラメータを読み込まなかった場合
	\begin{enumerate}
		\item 0以上9999以下の整数を標準入力から1つ受け取る。
		\item ランダムに初期化されたパラメータを用いて識別を行い、正解ラベルと認識結果を標準出力に出力する。
	\end{enumerate}
\end{enumerate}


\subsection*{実行結果}

my\_nn\_test.py において、課題3で中間層の活性化関数をシグモイド関数にして 30エポック学習させたパラメータを保存したファイル param\_30epoch.npz を標準入力から読み込ませ、（中間層の活性化関数をシグモイド関数、テスト回数を1としたうえで）0以上9999以下の値を何度か適当に選び、出力結果を確認した。

入力した整数と正解ラベル、および識別器の識別結果をまとめたものを以下の表 \ref{R1-1} に示す。

\begin{table}[H]
\begin{center}
\caption{入力と識別結果の一部}
  \begin{tabular}{|c|c|c|} \hline
    入力値 & 正解ラベル & 認識結果  \\ \hline \hline
    0 & 7 & 7 \\ \hline
    334 & 3 & 3 \\ \hline
    777 & 1 & 1 \\ \hline
    1000 & 9 & 9 \\ \hline
    1729 & 1 & 1 \\ \hline
    4126 & 8 & 8 \\ \hline
    5887 & 7 & 0 \\ \hline
    9999 & 6 & 6 \\ \hline
  \end{tabular}
	\label{R1-1}
\end{center}
\end{table}

また、MNIST テストデータ10000枚を用いて、バッチサイズ100、テスト回数1000回に設定してテストを行ったところ、認識精度は 91.11\% となっており、パラメータがうまく学習できていることが確認できた。

\subsection*{工夫点、問題点}
誰がプログラムを実行してもスムーズに進められるように、メッセージを充実させたり、認識結果と正解ラベルと画像を表示することによって、正しい識別が行われたかどうかを確認しやすくしたりしている。また、発展課題で行った実装のテストも行いやすいように汎用性を高めている。

\newpage

\section*{発展課題の実装内容}

\subsection*{A1: ReLU}

\subsubsection*{実装内容}

my\_nn\_learn.py, my\_nn\_test.py にReLU 関数を定義した。
ReLU 関数の順伝播の定義は、以下のソースコード \ref{A1-1} のようになっている。

\begin{lstlisting}[caption="ReLU 関数(順伝播)",label=A1-1]
def relu_forward(t):
    return np.maximum(t, 0)
\end{lstlisting}

また、ReLU の逆伝播を行う関数については以下のソースコード \ref{A1-2} のようになっている。

\begin{lstlisting}[caption="ReLU 関数(逆伝播)",label=A1-2]
def relu_backward(t):
    return np.where(t > 0, 1, 0)
\end{lstlisting}

必須課題においては中間層の活性化関数にシグモイド関数を用いていたが、この活性化関数を ReLU 関数に置き換えられるように my\_nn\_learn.py を実行した時に選択できるようにした。また、逆伝播についても、順伝播でReLUを用いた時は、ReLU 関数の微分に置き換えられる。ReLUの逆伝搬を行っている部分は、my\_nn\_learn.py の 540 行目にあたり、以下のソースコード \ref{A1-3} のようになっている。

\begin{lstlisting}[caption="ReLU の逆伝播 を行う部分",label=A1-3]
bp_data['g_activate_mid'] = np.dot(nn.network['w2'].T, bp_data['g_en_ak']) * relu_backward(data['a1'])
\end{lstlisting}

\subsubsection*{実行結果}

バッチサイズ $100$、学習係数 $0.01$、エポック数 $30$ 、最適化手法をSGDにし、活性化関数をシグモイド関数にしたものとReLU関数に設定したものをそれぞれ学習させた。なお、活性化関数をシグモイド関数にしたものは param\_sigmoid\_sgd\_30.npz、ReLU関数を用いたものは param\_relu\_sgd\_30.npz として提出したファイルに含めている。横軸に学習回数、縦軸にクロスエントロピー誤差の平均値をとったグラフを作成したところ、以下の図 \ref{fig-A-1-1} のようになった。

\image{ReLUとシグモイド関数の比較}{report_a1-1.png}{fig-A-1-1}

図 \ref{fig-A-1-1} の青い線がシグモイド関数を用いた時のクロスエントロピー誤差の平均値の値、橙色の線がReLU関数を用いた時の値である。図 \ref{fig-A-1-1} からReLU関数を用いた時も正しく学習が進んでいることと、シグモイド関数を用いた時よりも学習が早く、また学習用データに対しての認識精度もシグモイド関数を用いた時より上がっていることがわかる。

また、テストデータを用いて、param\_sigmoid\_sgd\_30.npz、param\_relu\_sgd\_30.npz のそれぞれに対して1000回テストを行ったところ認識精度は以下の表 \ref{table-A1-1} のようになった。

\begin{table}[H]
\begin{center}
\caption{テストデータでの認識精度(活性化関数の比較)}
  \begin{tabular}{|c|c|} \hline
    活性化関数 & 認識精度  \\ \hline \hline
    シグモイド & 91.052 \%  \\ \hline
    ReLU & 94.773 \% \\ \hline
  \end{tabular}
	\label{table-A1-1}
\end{center}
\end{table}

表 \ref{table-A1-1} から、ReLU関数の方がシグモイド関数を用いるより認識精度が高く、汎化性能が高いことがわかる。

\subsection*{A2: Dropout}

\subsubsection*{実装内容}

my\_nn\_learn.py, my\_nn\_test.py に Dropout を行うためのクラス Dropout を定義した。
学習時における Dropout クラスは my\_nn\_learn.py の79行目から定義されており、以下のソースコード \ref{A2-1} のようになっている。

\begin{lstlisting}[caption="クラス Dropout ",label=A2-1]
class Dropout:
    """ Class of Dropout

    Attributes:
        dropout_num: the node numbers that don't propagate the signal.
        mask: the mask of dropout.

    """
    # the rate of dropout
    rho = 0.5

    def __init__(self, nn):
        """ Initialize Dropout Class """
        self.dropout_num = np.random.choice(nn.m, int(nn.m * self.rho), replace=False)
        self.mask = np.zeros((nn.m, nn.batch_size))

    def gen_mask(self, nn):
        """ generating mask """
        tmp1 = np.identity(nn.m)[self.dropout_num]
        tmp2 = np.sum(tmp1, axis=0)
        tmp3 = np.repeat(tmp2, nn.batch_size)
        tmp4 = tmp3.reshape((nn.m, nn.batch_size))
        return 1 - tmp4

    def forward(self, nn, t):
        """ forwarding in Dropout """
        self.mask = self.gen_mask(nn)
        return t * self.mask

    def backward(self):
        """ back propagation in Dropout """
        return self.mask
\end{lstlisting}

ソースコード \ref{A2-1} の10行目の rho はパラメータであり、順伝播で無視するノードの割合（$ 0 < \rho < 1$)である。変数 nn.m は NNLearn クラスの変数で、中間層のノード数を表しており、nn.batch\_size はバッチサイズを表している。変数  dropout\_num は中間層のノード 0番〜 (nn.m)-1 番のうち伝搬しないノードの番号の値を格納しており、mask は順伝播、逆伝播で用いるマスクであり、(nn.m) 行 
 (nn.batch\_size) 列の行列で、中間層において伝播しないノードの行は全て0、他は1となっている。順伝播の時はこのマスクとaffine レイヤーからの入力のアダマール積を取ることによって活性化レイヤーに信号を伝播するかしないかを決定しており、逆伝播のときはこのマスク自体を微分した値として伝える。\\
 
 テスト時には、my\_nn\_test.py の221 行目にあるように、全てのノードを伝播するかわりに $1-\rho$ をかけている。
 
 \subsubsection*{実行結果}
 
バッチサイズ $100$、学習係数 $0.01$、エポック数 $30$ 、最適化手法をSGDにし、活性化関数をReLU関数にしたもの（ファイル名: param\_relu\_sgd\_30.npz ）とDropout に設定したもの（ファイル名: param\_dropout\_sgd\_30.npz ）をそれぞれ学習させた。なおDropout において、無視するノードの割合であるパラメータ$\rho$は$\rho = 0.5$とした。横軸に学習回数、縦軸にクロスエントロピー誤差の平均値をとったグラフを作成したところ、以下の図 \ref{fig-A-2-1} のようになった。

\image{ReLU関数とDropout の比較}{report_a2-1.png}{fig-A-2-1}

図 \ref{fig-A-2-1} の青い線がReLU関数を用いた時のクロスエントロピー誤差の平均値の値、橙色の線がDropoutを用いた時の値である。図 \ref{fig-A-2-1} から

また、テストデータを用いて、param\_relu\_sgd\_30.npz、param\_dropout\_sgd\_30.npz のそれぞれに対して1000回テストを行ったところ認識精度は以下の表 \ref{table-A1-1} のようになった。

\begin{table}[H]
\begin{center}
\caption{テストデータでの認識精度(ReLU と Dropout の比較)}
  \begin{tabular}{|c|c|} \hline
    活性化関数 & 認識精度  \\ \hline \hline
    ReLU & 94.773 \% \\ \hline
    Dropout & 91.77 \% \\ \hline
  \end{tabular}
	\label{table-A1-1}
\end{center}
\end{table}

表 \ref{table-A1-1} から、ReLU関数の方がシグモイド関数を用いるより認識精度が高く、汎化性能が高いことがわかる。
 
\subsection*{A4-1: 最適化手法の改良...Momentum SGD}

\subsubsection*{実装内容}
my\_nn\_learn.py に新しく momentum\_sgd 関数を定義した。momentum\_sgd 関数の内容は以下のソースコード \ref{A4-1-1}の通りである。
\begin{lstlisting}[caption="Momentum SGD",label=A4-1-1]
def momentum_sgd(nn, bp_data):
    nn.bp_param['msgd_w1'] = (nn.alpha * nn.bp_param['msgd_w1']) - (nn.eta * bp_data['g_en_w1'])
    nn.bp_param['msgd_w2'] = (nn.alpha * nn.bp_param['msgd_w2']) - (nn.eta * bp_data['g_en_w2'])
    nn.network['w1'] += nn.bp_param['msgd_w1']
    nn.network['w2'] += nn.bp_param['msgd_w2']
    nn.network['b1'] -= nn.eta * bp_data['g_en_b1']
    nn.network['b2'] -= nn.eta * bp_data['g_en_b2']
\end{lstlisting}

この関数を、誤差逆伝播法の勾配計算の後に、SGDの代わりに適用した。

また、クラス NNLearn 内にパラメータ $\alpha, \Delta W$ の初期値を記入できるようにした。学習率 $\eta$ については SGD のときと共用で用いる。
\begin{lstlisting}[caption="Momentum SGD",label=A4-1-1]
class NNLearn:
    # --- for Momentum SGD ---
    alpha = 0.9
    bp_param = {}
    bp_param['msgd_w1'] = np.zeros((m, d))
    bp_param['msgd_w2'] = np.zeros((c, m))
\end{lstlisting}
\subsubsection*{実行結果}

活性化関数を ReLU、最適化手法を Momentum SGD にして、バッチサイズ 100、パラメータの初期値を$\eta = 0.01, \alpha = 0.9$ に設定して 30 エポック学習させた。図 \ref{fig-A4-1-1} は学習回数と、クロスエントロピー誤差の平均値の変化のグラフである。なお、比較用に、最適化手法を SGD にしてその他の条件を同じにして学習させた時のクロスエントロピー誤差の平均値もプロットしてある。

\image{SGDとMomentum SGDの比較}{report_a4-1.png}{fig-A4-1-1}

図\ref{fig-A4-1-1} の青色の線が最適化手法を SGD にしたもの、桃色の線が Momentum SGD にしたものである。図\ref{fig-A4-1-1} から、Momentum SGD の方が学習が早く、また、学習の終盤において、SGDよりもクロスエントロピー誤差の平均値の振幅が小さくなっていること、および、クロスエントロピー誤差の平均値の値が SGD の値を用いた時よりも小さくなっていることがわかる。

\subsection*{A4-2: 最適化手法の改良...AdaGrad}

\subsection*{A4-3: 最適化手法の改良...RMSProp}

\subsection*{A4-4: 最適化手法の改良...AdaDelta}

\subsection*{A4-5: 最適化手法の改良...Adam}

\subsection*{A4 最適化手法の改良...比較}

テスト用データを用いて、バッチサイズ100、テスト回数100にしてそれぞれの最適化手法でテストを行ったところ、認識精度は表\ref{tableA4} のようになった。\\

\begin{table}[H]
\begin{center}
\caption{テストデータでの認識精度(最適化手法での比較)}
  \begin{tabular}{|c|c|} \hline
    最適化手法 & 認識精度  \\ \hline \hline
    SGD & 94.773 \% \\ \hline
    Momentum SGD & 97.930 \% \\ \hline
  \end{tabular}
	\label{tableA4}
\end{center}
\end{table}

\subsection*{A5: カラー画像への拡張}

\subsubsection*{実装内容}

新たにカラー画像の学習用のプログラムファイル my\_nn\_color\_learn.py とテスト用プログラムファイル my\_nn\_color\_test.py を用意した。実装内容は以下の通りである。

\begin{itemize}
	\item モノクロ画像では Python3 系で学習させていたが、cPickle を用いるため Python2 系で実装を行った
	\item 次元数を $ d=1 \times 28 \times 28 $ から $ d=3 \times 32 \times 32 $ に変更
	\item 中間層のノード数を 1000 に変更
	\item 中間層の活性化関数は ReLU
	\item 逆伝播後の最適化手法は Adam を採用
	\item Adam のパラメータは $\alpha = 0.001, \beta_1 = 0.9, \beta_2 =  0.999, \epsilon = 10^{-8}$ に設定
\end{itemize}

\end{document}
